{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d533047",
   "metadata": {},
   "source": [
    "# Homework - Neural networks - Part E (50 points)\n",
    "## Discovering lexical classes from simple sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ce9501",
   "metadata": {},
   "source": [
    "by *Brenden Lake* and *Todd Gureckis*  \n",
    "Computational Cognitive Modeling  \n",
    "NYU class webpage: https://brendenlake.github.io/CCM-site/  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a33a7e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\" role=\"alert\">\n",
    "This homework is due before midnight on Monday, Feb. 13, 2023.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110537d8",
   "metadata": {},
   "source": [
    "In this assignment, you will follow in Elman's (1990) footsteps by coding and training a Simple Recurrent Network (SRN) on a set of simple sentences. \n",
    "- **Before training**, the SRN can process sequences but otherwise knows nothing about language. Initially, it represents each word as an arbitrary continuous vector (input embedding) without knowledge of their roles or how they relate to each other.\n",
    "- **During training**, the SRN aims to predict the next word in a sentence given the previous words. The optimizer takes a step after each sentence.\n",
    "- **After training**, you will analyze the SRN's internal representations (input embeddings) for evidence that it has discovered something about lexical classes (e.g., nouns and verbs).\n",
    "\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "Reference (available for download on Brightspace):\n",
    "    \n",
    "Elman, J. L. (1990). Finding Structure in Time. Cognitive Science, 14:179â€“211.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1b0e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start with some packages we need\n",
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0e6487",
   "metadata": {},
   "source": [
    "### Elman's set of simple sentences\n",
    "The training set consists of 10,000 sentences each with 2 or 3 words. Elman generated each sentence as follows:\n",
    "1. Choose one of 16 templates specificying a sequence of lexical classes (see below).\n",
    "2. Each lexical class is replaced by a word sampled from that class (see below, only a subset of words shown).\n",
    "\n",
    "The vocabulary contained 29 words. For example, the template `NOUN-AGRESS VERB-EAT NOUN-FOOD` can lead to the sentence `dragon eat cookie` along with other possibilities. We generated 10,000 sentences using our best guess of Elman's procedure (the full set of lexical classes isn't listed). You can see these sentences in the external text file `data/elman_sentences.txt`\n",
    "\n",
    "<img src=\"images/elman_tab3.png\" width=400px>\n",
    "<img src=\"images/elman_tab4.png\" width=400px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82decfce",
   "metadata": {},
   "source": [
    "### Loading the data\n",
    "The following code will load and process the set of simple sentences. As is common in neural networks for text and natural language processing, the sentence strings are first \"tokenized\" into a list of discrete elements (words in this case). Additionally, special tokens indicating the start-of-sentence `<SOS>`  and end-of-sentence `<EOS>`  are added at the beginning and end of the sentence, respectively. The SRN requires an input at every step and thus we use `<SOS>` as the first input when the SRN is predicting the first word as output. The SRN can self-terminate a sentence by producing `<EOS>` as an output. The dict `token_to_index` maps each token to a unique integer, which is the format that the SRN actually uses as input.\n",
    "\n",
    "Running the code below will show you the dict `token_to_index` and how the first sentence `dragon break plate` is tokenized into integers. Make sure you understand how this works and how to map back and forth between the formats!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6419fdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentenceToTensor(tokens_list):\n",
    "    # Convert list of strings to tensor of token indices (integers)\n",
    "    #\n",
    "    # Input\n",
    "    #  tokens_list : list of strings, e.g. ['<SOS>','lion','eat','man','<EOS>']\n",
    "    # Output\n",
    "    #  1D tensor of the same length (integers), e.g., tensor([ 2, 18, 13, 19,  0])\n",
    "    assert(isinstance(tokens_list,list))\n",
    "    tokens_index = [token_to_index[token] for token in tokens_list]\n",
    "    return torch.tensor(tokens_index)\n",
    "\n",
    "# load and process the set of simple sentences\n",
    "with open('data/elman_sentences.txt','r') as fid:\n",
    "    lines = fid.readlines()\n",
    "sentences_str = [l.strip() for l in lines]\n",
    "sentences_tokens = [s.split() for s in sentences_str]\n",
    "sentences_tokens = [['<SOS>']+s+['<EOS>'] for s in sentences_tokens]\n",
    "unique_tokens = sorted(set(sum(sentences_tokens,[])))\n",
    "n_tokens = len(unique_tokens) # all words and special tokens\n",
    "token_to_index = {t : i for i,t in enumerate(unique_tokens)}\n",
    "index_to_token = {i : t for i,t in enumerate(unique_tokens)}\n",
    "training_pats = [sentenceToTensor(s) for s in sentences_tokens] # python list of 1D sentence tensors\n",
    "ntrain = len(training_pats)\n",
    "print('mapping unique tokens to integers: %s \\n' % token_to_index)\n",
    "print('example sentence as string: %s \\n' % ' '.join(sentences_tokens[0]))\n",
    "print('example sentence as tensor: %s \\n' % training_pats[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaad770c",
   "metadata": {},
   "source": [
    "### Simple Recurrent Network\n",
    "The diagram below shows the unrolled SRN that you will develop here. As is always true for recurrent networks, notice the tied weights $U$, $W$, $V$, etc.\n",
    "<img src=\"images/elman_sent_srn.png\" width=500px>\n",
    "We will deviate from Elman's exact model in a few ways to make it more modern. Here is the specification we will use. \n",
    "- **Input embedding**. In Elman's original model, each word was represented by a fixed one-hot input vector. Instead, here we will learn a continuous embedding vector (size `hidden_size=20`) to represent each input word. These vectors are learnable parameters. When a word is provided as input to the SRN, it is converted to the corresponding input embedding. This layer is setup for you already in the started class, `self.embed = nn.Embedding(vocab_size,hidden_size)`\n",
    "- **Hidden layer**. This layer has length `hidden_size` and uses the **logistic** activation function. The initial vector $h_{-1}$ should be all zeros.\n",
    "- **Output layer**. This layer has length `vocab_size` and uses the **softmax** activation function. Thus, the SRN will represent an explicit probability distribution over the next token $w_j$ given the past tokens $w_1,\\dots,w_{j-1}$, through the equation $P(w_j | w_1,\\dots,w_{j-1})$\n",
    "- **Loss**. The SRN will train to maximize the log-likelihood of the target output words, e.g., we use the negative log-likelihood loss `nn.NLLLoss`. If passed a tensor representing multiple target predictions, this loss takes the mean across predictions.\n",
    "- **Optimizer**. We found reasonable results with the `AdamW` optimizer with weight decay of 0.04. Adam is like stochastic gradient descent but adapts the learning rate for each parameter based on the variance of the gradient. Weight decay encourages the parameters to be close to zero leading to more stable input embeddings.\n",
    "- **Batching**. We suggest *no batching* for this simple code. Thus, the optimizer takes a step after each individual sentence. The `forward` method should process only one input word at a time. Batching produces much faster code and is recommended in practice, but it's not required here. If you want to rewrite the code to process multiple timesteps and sentences simultaneously, that's fine too."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78584536",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "<h3> Problem 1 (20 points) </h3>\n",
    "<br>\n",
    "Write code to complete the SRN class.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe40b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRN(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        # vocab_size : number of tokens in vocabulary including special tokens <SOS> and <EOS>\n",
    "        # hidden_size : dim of input embeddings and hidden layer\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embed = nn.Embedding(vocab_size,hidden_size)\n",
    "        # TODO : YOUR CODE GOES HERE\n",
    "        raise Exception('Replace with your code.')\n",
    "\n",
    "    def forward(self, input_token_index, hidden_prev):\n",
    "        # Input\n",
    "        #    input_token_index: [integer] index of current input token\n",
    "        #    hidden_prev: [length hidden_size 1D tensor] hidden state from previous step\n",
    "        # Outpuut\n",
    "        #    output: [length vocab_size 1D tensor] log-probability of emitting each output token\n",
    "        #    hidden_curr : [length hidden_size 1D tensor] hidden state for current step\n",
    "        input_embed = self.embed(input_token_index) # hidden_size 1D tensor\n",
    "        # TODO : YOUR CODE GOES HERE\n",
    "        raise Exception('Replace with your code.')\n",
    "        return output, hidden_curr\n",
    "\n",
    "    def initHidden(self):\n",
    "        # Returns length hidden_size 1D tensor of zeros\n",
    "        return torch.zeros(self.hidden_size)\n",
    "    \n",
    "    def get_embeddings(self):\n",
    "        # Returns [vocab_size x hidden_size] numpy array of input embeddings\n",
    "        return self.embed(torch.arange(self.vocab_size)).detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21839809",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "<h3> Problem 2 (20 points) </h3>\n",
    "<br>\n",
    "Write code to complete the `train` function and the main training loop. In the training loop, for each epoch, print out the mean loss over all training patterns. An epoch should visit each sentence in random order, taking an optimizer step after each sentence.\n",
    "</div>\n",
    "\n",
    "**Hint:** In my implementation, after 10 epochs, I found that the mean loss to reach about 1.57. In other words, the SRN predicts the right word with roughly $e^{-1.57}=0.208$ probability of getting it right. (Of course, perfect prediction is impossible in even this simple language)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b6b7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(seq_tensor, rnn):\n",
    "    # Process a sentence and update the SRN weights. With <SOS> as the input at step 0,\n",
    "    # predict every subsequent word given the past words.\n",
    "    # Return the mean loss across each symbol prediction.\n",
    "    #\n",
    "    # Input\n",
    "    #   seq_tensor: [1D tensor] sentence as token indices\n",
    "    #   rnn : instance of SRN class\n",
    "    # Output\n",
    "    #   loss : [scalar] average NLL loss across prediction steps\n",
    "\n",
    "    # TODO : YOUR CODE GOES HERE\n",
    "    raise Exception('Replace with your code.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c0c754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main training loop\n",
    "nepochs = 10 # number of passes through the entire training set \n",
    "nhidden = 20 # number of hidden units in the SRN\n",
    "rnn = SRN(n_tokens,nhidden)\n",
    "optimizer = torch.optim.AdamW(rnn.parameters(), weight_decay=0.04) # w/ default learning rate 0.001\n",
    "criterion = nn.NLLLoss()\n",
    "# TODO : YOUR CODE GOES HERE\n",
    "raise Exception('Replace with your code.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29604f26",
   "metadata": {},
   "source": [
    "### Analyze the SRN internal representations\n",
    "Once training is done, we want to examine the internal representations to see what the network has learned about the lexical items. Elman ran a hierarchical clustering analysis using the mean hidden representation of each word when presented across the corpus. \n",
    "<img src=\"images/elman_fig7.png\" width=500px>\n",
    "\n",
    "Unlike Elman we have an **explicit input embedding** for each word, and thus we can more simply look at these embedding vectors. Run the code to compare with Elman's results. *You shouldn't expect a close match.* There are differences in network architecture, training, and the dataset. Still, it's interesting to see what your SRN has learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7455dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dendo(X, names, exclude=['<SOS>','<EOS>']):\n",
    "    #  Show hierarchical clustering of vectors \n",
    "    #\n",
    "    # Input\n",
    "    #  X : numpy tensor [nitem x dim] such that each row is a vector to be clustered\n",
    "    #  names : [length nitem] list of item names\n",
    "    #  exclude: list of names we want to exclude       \n",
    "    nitem = len(names)\n",
    "    names  = np.array(names)\n",
    "    include = np.array([myname not in exclude for myname in names], dtype=bool)\n",
    "    linked = linkage(X[include],'single', optimal_ordering=True)\n",
    "    plt.figure(1, figsize=(20,6))\n",
    "    dendrogram(linked, labels=names[include], color_threshold=0, leaf_font_size=18)\n",
    "    plt.show()\n",
    "\n",
    "plot_dendo(rnn.get_embeddings(), unique_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88d44b0",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "<h3> Problem 3 (10 points) </h3>\n",
    "<br>\n",
    "Write a function `generate` to probabilistically sample sentences from your network. Generate 10 sample sentences in this manner. For each, convert the sequence of token indices back to string form. When printing the sentence, you can either include the SOS and EOS or ignore them. It's fine to assume a maximum length.\n",
    "</div>\n",
    "\n",
    "**Hint:** You will find `torch.distributions.categorical.Categorical` useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d38772e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(rnn, maxlen=4):\n",
    "    # TODO : YOUR CODE GOES HERE\n",
    "    raise Exception('Replace with your code.')\n",
    "    \n",
    "for i in range(10):\n",
    "    generate(rnn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
